{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ee\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a Sentinel-2 collection\n",
    "#this is initialised for each day/period of analysis\n",
    "#Rob has altered this to NOT filter by clouds (we want that statistic in our outputs)\n",
    "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "        #.filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
    "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "        'primary': s2_sr_col,\n",
    "        'secondary': s2_cloudless_col,\n",
    "        'condition': ee.Filter.equals(**{\n",
    "            'leftField': 'system:index',\n",
    "            'rightField': 'system:index'\n",
    "        })\n",
    "    }))\n",
    "\n",
    "def add_required_bands(img):\n",
    "    #This shows how we wold select an individual band and rename it\n",
    "    #green = img.select('B3').rename('green')\n",
    "    \n",
    "    ndvi = img.normalizedDifference(['B8','B4']).rename('ndvi')\n",
    "    #ndwi = img.normalizedDifference(['B3','B8']).rename('ndwi')#['B3','B8']#.float()\n",
    "    mndwi = img.normalizedDifference(['B3','B12']).rename('mndwi')\n",
    "    ndmi = img.normalizedDifference(['B8','B11']).rename('ndmi')\n",
    "    \n",
    "    #This was Rob manually performing calcs, just use the normalizedDifference\n",
    "    #ndwi = green.subtract(nir).divide(green.add(nir)).rename('ndwi')\n",
    "    #mndwi = green.subtract(swir2).divide(green.add(swir2)).rename('mndwi')\n",
    "    \n",
    "    #Add our bands to the original collection\n",
    "    return img.addBands(ee.Image([ndvi, mndwi, ndmi]))#,ndwi\n",
    "\n",
    "#Define cloud mask component functions\n",
    "#Cloud Components\n",
    "#Pretty much unchanged from examples on web\n",
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))\n",
    "\n",
    "#Cloud Shadow Components\n",
    "#Pretty much unchanged from examples on web\n",
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select('SCL').neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
    "        .select('distance')\n",
    "        .mask()\n",
    "        .rename('cloud_transform'))\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n",
    "\n",
    "#Final cloud shadow mask\n",
    "#Rob has altered the application of a threshold\n",
    "#Rob has also used this function to add in our required bands\n",
    "def add_cld_shdw_mask(img):\n",
    "    \n",
    "    #Add Robs NDWI stuff\n",
    "    img = img#.clip(AOI)#Clipping here doesn't really speed things up\n",
    "    \n",
    "    addReqBands = add_required_bands(img)\n",
    "    \n",
    "    # Add cloud component bands.\n",
    "    #img_cloud = add_cloud_bands(img)\n",
    "    img_cloud = add_cloud_bands(addReqBands)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0).rename('cloudandshadow')\n",
    "    #is_cld_shdw = is_cld_shdw.rename('cloudandshadow')\n",
    "    \n",
    "    #Rob thinks this is cutting out too much cloud....\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw_FB = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20}))\n",
    "        #.rename('cloudmasksimple'))\n",
    "\n",
    "    is_cld_shdw_Buff = (is_cld_shdw.focalMax(BUFFERSMALL*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20}))\n",
    "    \n",
    "    is_cld_shdw_FB = is_cld_shdw_FB.rename('cloudandshadowfiltbuff')\n",
    "    is_cld_shdw_Buff = is_cld_shdw_Buff.rename('cloudandshadowbuff')\n",
    "    \n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    #return img_cloud_shadow.addBands(is_cld_shdw, is_cld_shdw_simple)\n",
    "    return img_cloud_shadow.addBands(ee.Image([is_cld_shdw, is_cld_shdw_FB, is_cld_shdw_Buff]))\n",
    "\n",
    "#This is the analysis applied to each 'day', assuming the collection has already been built (with our bands added)\n",
    "def calc_stats_from_layers(col, AOI, json_storages, utmZoneInfo, mndwiMaskVal, ndviMaskVal):\n",
    "\n",
    "    mndwiMaskVal = float(mndwiMaskVal)\n",
    "    ndviMaskVal = float(ndviMaskVal)\n",
    "\n",
    "    # Mosaic the image collection.\n",
    "    img = col.mosaic()#.clip(AOI)\n",
    "    \n",
    "    #selfMask, I think, drops out Zeros, just like it you manually masked to keep values > 0\n",
    "\n",
    "    # Subset layers and prepare them for display.\n",
    "    #clouds = img.select('clouds').selfMask()\n",
    "    #shadows = img.select('shadows').selfMask()\n",
    "    #dark_pixels = img.select('dark_pixels').selfMask()\n",
    "    #probability = img.select('probability')\n",
    "    #cloudandshadow = img.select('cloudandshadow').selfMask()\n",
    "    #cloudandshadowfandb = img.select('cloudandshadowfiltbuff').selfMask()\n",
    "    cloudandshadowbuff = img.select('cloudandshadowbuff').selfMask()\n",
    "    #cloud_transform = img.select('cloud_transform')\n",
    "\n",
    "\n",
    "    ndvi = img.select('ndvi')\n",
    "    ndviMaskLayer = ndvi.gte(ndviMaskVal).rename('ndviMaskLayer')\n",
    "    mndwi = img.select('mndwi')\n",
    "    mndwiMaskLayer = mndwi.gte(mndwiMaskVal).rename('mndwiMaskLayer')\n",
    "    ndmi = img.select('ndmi')\n",
    "    \n",
    "    #This would retain values of MNDWI, not set them to 0/1\n",
    "    #mndwimasked = mndwi.updateMask(mndwiMaskLayer)#.rename('ndwi')\n",
    "\n",
    "    \n",
    "    #Perform reprojections\n",
    "    ndviProj = ndvi.clip(AOI).reproject(crs=utmZoneInfo, scale=10)\n",
    "    mndwiProj = mndwi.clip(AOI).reproject(crs=utmZoneInfo, scale=10)#Chosen 10 metres, , crsTransform='null'\n",
    "    ndmiProj = ndmi.clip(AOI).reproject(crs=utmZoneInfo, scale=10)#Chosen 10 metres, , crsTransform='null'\n",
    "    #ndwiMaskedProj = ndwiMaskLayer.clip(AOI).reproject(crs=utmZoneInfo, scale=10)\n",
    "    ndviMaskedProj = ndviMaskLayer.clip(AOI).reproject(crs=utmZoneInfo, scale=10)\n",
    "    mndwiMaskedProj = mndwiMaskLayer.clip(AOI).reproject(crs=utmZoneInfo, scale=10)\n",
    "    cloudandshadowbuffProj = cloudandshadowbuff.clip(AOI).reproject(crs=utmZoneInfo, scale=10)\n",
    "\n",
    "    #This will now give each cell in the mask area 1 * pixel area\n",
    "    #Need one complete raster for total area stats calc\n",
    "    allStatsLayer = mndwiProj.gte(-50.0).multiply(ee.Image.pixelArea()).rename('allPixels')\n",
    "    \n",
    "    ndviStatsLayer = ndviMaskedProj.multiply(ee.Image.pixelArea()).rename('ndviStatsLayer')\n",
    "    ndviAvgLayer = ndviProj.multiply(1).rename('ndviAvgLayer')\n",
    "\n",
    "    mndwiStatsLayer = mndwiMaskedProj.multiply(ee.Image.pixelArea()).rename('mndwiStatsLayer')\n",
    "    mndwiAvgLayer = mndwiProj.multiply(1).rename('mndwiAvgLayer')\n",
    "    \n",
    "    ndmiAvgLayer = ndmiProj.multiply(1).rename('mndwiAvgLayer')\n",
    "    \n",
    "    cloudStatsLayer = cloudandshadowbuffProj.multiply(ee.Image.pixelArea()).rename('cloudStatsLayer')\n",
    "\n",
    "    #DF to capture stats\n",
    "    startStats = datetime.datetime.now()\n",
    "    \n",
    "    #print(\"creating collection\")\n",
    "    theCollection = ee.FeatureCollection(json_storages)\n",
    "    #print(\"created collection\")\n",
    "\n",
    "    #A bunch of server-side ee.FeatureCollections...\n",
    "    #Use getInfo to transfer server-side feature collection to the client. The result is an object.\n",
    "    #The .getInfo()['features']) should drag the results back, client side????\n",
    "    #These are a bunch of JSON objects, unfortunately they include the coordinates of every feature!!!!\n",
    "    #print(\"reducing for features\")\n",
    "    allStats = allStatsLayer.reduceRegions(**{'reducer': ee.Reducer.sum(),'crs':utmZoneInfo, 'scale': 10,'collection': theCollection}).getInfo()['features']\n",
    "    #ndwiStats = ndwiStatsLayer.reduceRegions(**{'reducer': ee.Reducer.sum(),'crs':utmZoneInfo, 'scale': 10,'collection': theCollection}).getInfo()['features']\n",
    "    mndwiStats = mndwiStatsLayer.reduceRegions(**{'reducer': ee.Reducer.sum(),'crs':utmZoneInfo, 'scale': 10,'collection': theCollection}).getInfo()['features']\n",
    "    mndwiAvg = mndwiAvgLayer.reduceRegions(**{'reducer': ee.Reducer.mean(),'crs':utmZoneInfo, 'scale': 10,'collection': theCollection}).getInfo()['features']\n",
    "    ndviStats = ndviStatsLayer.reduceRegions(**{'reducer': ee.Reducer.sum(),'crs':utmZoneInfo, 'scale': 10,'collection': theCollection}).getInfo()['features']\n",
    "    ndviAvg = ndviAvgLayer.reduceRegions(**{'reducer': ee.Reducer.mean(),'crs':utmZoneInfo, 'scale': 10,'collection': theCollection}).getInfo()['features']\n",
    "    ndmiAvg = ndmiAvgLayer.reduceRegions(**{'reducer': ee.Reducer.mean(),'crs':utmZoneInfo, 'scale': 10,'collection': theCollection}).getInfo()['features']\n",
    "    cloudStats = cloudStatsLayer.reduceRegions(**{'reducer': ee.Reducer.sum(),'crs':utmZoneInfo, 'scale': 10,'collection': theCollection}).getInfo()['features']\n",
    "    #print(\"reduced for features\")\n",
    "\n",
    "    endStats = datetime.datetime.now()\n",
    "    statsDdiff = (endStats-startStats).total_seconds()\n",
    "\n",
    "    #print(\"Zonal Stats took \" + str(statsDdiff) + \" seconds\")\n",
    "    #print(ndwiStats.getInfo()['features'][0]['properties'])\n",
    "\n",
    "    #Send back a dictionary of JSON objects, these string identifiers will need to be match in the script that calls this function\n",
    "    return {'allAreas':allStats,'ndviAreas':ndviStats,'mndwiAreas':mndwiStats,'ndviAvg':ndviAvg,'mndwiAvg':mndwiAvg,'ndmiAvg':ndmiAvg,'cloudAreas':cloudStats}\n",
    "\n",
    "#Function to pull out our metrics from GEE JSON results\n",
    "def retrieveStatsFromJSON(theResultsDict, theIDField, theMetric, theDataPropertyName):\n",
    "    #This is an example output\n",
    "    #[{'type': 'Feature', 'geometry': {'type': 'Polygon', 'coordinates': [[[151.29950364947237, -27.748655793369675], [151.31082715269562, -27.752316677570064]]]},\n",
    "    #'id': '0', 'properties': {'ADD_2019': None, 'AUTH_REF': None, 'AUTH_STAT': None, 'AVG_DEPTH': None, 'COMPL_ACT': None, 'COMPL_ID': None, 'FEATURE': 'notified', 'FSL': None, 'GIS_CAP': None, 'LIDAR_CAP': None, 'LOT_PLAN': None, 'NOW_CAP': None, 'NOW_FMDBID': None, 'NOW_FMID': None, 'NOW_PROV': None, 'NOW_RECD': None, 'NOW_WKDBID': '23649', 'OBJECTID': 12769, 'OFFICE': 'Warwick', 'OWNER_2019': None, 'OWNER_2020': None, 'REMARKS': None, 'STR_STATUS': None, 'STR_TYPE': 'storage', 'SUBCAT': None, 'WATERPLAN': 'Condamine and Balonne', 'YR_CAPTURE': None, 'sum': 438920.0014711266}}]\n",
    "    #print(\"The length: \" + str(len(theResultsDict)))\n",
    "    geeStats = pd.DataFrame(columns=[theIDField, theDataPropertyName])\n",
    "    #print(theDataPropertyName)\n",
    "    #print(theIDField)\n",
    "    #print(theMetric)\n",
    "    for i in range(len(theResultsDict)):\n",
    "        #print(theResultsDict[i][\"properties\"])\n",
    "        #print(type(theResultsDict[i][\"properties\"]))\n",
    "        #keys = list(theResultsDict[i][\"properties\"].keys())\n",
    "        #print(keys)\n",
    "        theID = theResultsDict[i][\"properties\"][theIDField]\n",
    "        #print(theID)\n",
    "        dataRow = [theID, -9999]\n",
    "        #print(\"Before check\")\n",
    "        #print(dataRow)\n",
    "        if theMetric in theResultsDict[i][\"properties\"]:\n",
    "        #    print(\"In here first\")\n",
    "        #    print(dataRow)\n",
    "            dataRow = [theID, theResultsDict[i][\"properties\"][theMetric]]\n",
    "        #    print(dataRow)\n",
    "        \n",
    "        geeStats.loc[len(geeStats)] = dataRow\n",
    "    #print(geeStats)\n",
    "    return geeStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup proxy server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yourUserName = \"dechastelj\" #\"ellisr\"\n",
    "yourPassword = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['http_proxy'] = \"http://\" + yourUserName + \":\" + yourPassword + \"@web-prdproxy-usr.dmz:80\"\n",
    "os.environ['https_proxy'] = \"http://\" + yourUserName + \":\" + yourPassword + \"@web-prdproxy-usr.dmz:80\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=d8eCcZovNNtU9hZj2IJ8pemGtlnH7k83QLdpGAxAGqY&tc=hicKRR5x__QYCbQmslrW_AfluQZ1CZ0V-5wTIP3HDRQ&cc=3_BZSCaNjgyIUHq4iJe0srGJN-AQY-PkFpUeQLpmpMo>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=d8eCcZovNNtU9hZj2IJ8pemGtlnH7k83QLdpGAxAGqY&tc=hicKRR5x__QYCbQmslrW_AfluQZ1CZ0V-5wTIP3HDRQ&cc=3_BZSCaNjgyIUHq4iJe0srGJN-AQY-PkFpUeQLpmpMo</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: 4/1AbUR2VPUDUf0-X8QvoKUU2W3L1If6XLQooY2JtDUkD7ngCmFvxxACIpjZLQ\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "#Don't need this every time, initialize can be enough\n",
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your shapefile, we'll reproject to WGS84 for transfer to GEE\n",
    "\n",
    "#The shapefile of features\n",
    "featurePath1 = r\"C:\\Users\\dechastelj\\Jupyter\\BR_storages_WGS84.shp\"\n",
    "featurePath2 = r\"C:\\Users\\dechastelj\\Jupyter\\BR_cropping_final_WGS84.shp\"\n",
    "featurePath3 = r\"C:\\Users\\dechastelj\\Jupyter\\BR_sample_parcels_WGS84.shp\"\n",
    "featurePathList = [featurePath1] #Just storages\n",
    "\n",
    "#The Field with the identifier\n",
    "uniqueid_field = \"UNIQUEID\"\n",
    "\n",
    "#The most appropriate UTM zone for spatial analysis\n",
    "#utmZoneInfo = \"EPSG:28356\"#z56\n",
    "utmZoneInfo = \"EPSG:28355\"#z55\n",
    "#utmZoneInfo = \"EPSG:28354\"#z54\n",
    "\n",
    "#Output CSV to hold hard copy\n",
    "outputCSV = r\"C:\\Users\\dechastelj\\Jupyter\\output\\BR_sample_parcels_output.csv\"\n",
    "\n",
    "#Start of analysis period, YYYY-MM-DD\n",
    "#sDate = \"2019-01-01\"\n",
    "sDate = \"2023-02-21\"\n",
    "\n",
    "#End of analysis period, YYYY-MM-DD\n",
    "eDate = \"2023-02-22\"\n",
    "\n",
    "#Index threshold Values for area (greater than or equal to)\n",
    "mdwiThresh = 0.0\n",
    "ndviThresh = 0.5\n",
    "\n",
    "CLOUD_FILTER = 60\n",
    "CLD_PRB_THRESH = 50\n",
    "NIR_DRK_THRESH = 0.15\n",
    "CLD_PRJ_DIST = 1\n",
    "BUFFER = 50\n",
    "BUFFERSMALL = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count total days for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total days: 8\n"
     ]
    }
   ],
   "source": [
    "overallStartDate = datetime.datetime.strptime(sDate,\"%Y-%m-%d\")\n",
    "overallEndDate = datetime.datetime.strptime(eDate,\"%Y-%m-%d\")\n",
    "delta = overallEndDate - overallStartDate\n",
    "\n",
    "\n",
    "theDayCount = delta.days + 1\n",
    "\n",
    "print(\"Total days:\", theDayCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_shapes = gpd.read_file(featurePath)\n",
    "#feature_shapesWGS84 = feature_shapes.to_crs(4326)\n",
    "#json_features = json.loads(feature_shapesWGS84.to_json())\n",
    "\n",
    "#This will give bounds in expected GEE format ### use feature geometry instead to avoid empty days\n",
    "#geeFeatureGeometry = ee.Geometry(ee.FeatureCollection(json_features).geometry())\n",
    "#AOI = geeFeatureGeometry.bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the ID Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the unique quality code values\n",
    "#print(feature_shapes[uniqueid_field].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create string variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis\n",
    "#some string contstants\n",
    "dateString = \"date\"\n",
    "totAreaString = \"total_area\"\n",
    "mdwiAreaString = \"mdwi_area\"\n",
    "ndwiAreaString = \"ndwi_area\"\n",
    "mdwiAvgString = \"mdwi_avg\"\n",
    "ndviAreaString = \"ndvi_area\"\n",
    "ndviAvgString = \"ndvi_avg\"\n",
    "ndmiAvgString = \"ndmi_avg\"\n",
    "cloudAreaString = \"cloud_area\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 11/05/2023, 10:56:38\n",
      "\n",
      "Starting features: C:\\Users\\dechastelj\\Jupyter\\BR_storages_WGS84.shp\n",
      "Processing period: 2023-02-21 at 11/05/2023, 10:56:38\n",
      "Images found: 3\n",
      "Done processing day: 2023-02-21 at 11/05/2023, 10:57:12\n",
      "\n",
      "Processing period: 2023-02-22 at 11/05/2023, 10:57:12\n",
      "No images found\n",
      "\n",
      "Processing period: 2023-02-23 at 11/05/2023, 10:57:13\n",
      "No images found\n",
      "\n",
      "Processing period: 2023-02-24 at 11/05/2023, 10:57:15\n",
      "No images found\n",
      "\n",
      "Processing period: 2023-02-25 at 11/05/2023, 10:57:17\n",
      "No images found\n",
      "\n",
      "Processing period: 2023-02-26 at 11/05/2023, 10:57:20\n",
      "Images found: 3\n",
      "Done processing day: 2023-02-26 at 11/05/2023, 10:57:54\n",
      "\n",
      "Processing period: 2023-02-27 at 11/05/2023, 10:57:54\n",
      "No images found\n",
      "\n",
      "Processing period: 2023-02-28 at 11/05/2023, 10:57:56\n",
      "No images found\n",
      "\n",
      "Entire period processed, started at 11/05/2023, 10:56:38 and finished at 11/05/2023, 10:57:58\n"
     ]
    }
   ],
   "source": [
    "#Empty dataframe for results\n",
    "tsStats = pd.DataFrame(columns=[uniqueid_field, dateString, totAreaString, mdwiAreaString,\n",
    "                                     mdwiAvgString, ndviAreaString, ndviAvgString, ndmiAvgString, cloudAreaString])\n",
    "\n",
    "# Get current time\n",
    "beginNow = datetime.datetime.now().strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "print(\"Starting at \" + beginNow)\n",
    "print(\"\")\n",
    "\n",
    "# for each feature\n",
    "for featurePath in featurePathList:\n",
    "    \n",
    "    # Print feature name\n",
    "    print(\"Starting features:\", featurePath)\n",
    "    \n",
    "    # Get feature geometry    \n",
    "    feature_shapes = gpd.read_file(featurePath)\n",
    "    feature_shapesWGS84 = feature_shapes.to_crs(4326)\n",
    "    json_features = json.loads(feature_shapesWGS84.to_json())\n",
    "\n",
    "    #This will give bounds in expected GEE format ### use feature geometry instead to avoid empty days\n",
    "    geeFeatureGeometry = ee.Geometry(ee.FeatureCollection(json_features).geometry())\n",
    "    #AOI = geeFeatureGeometry.bounds()\n",
    "\n",
    "    # For each day\n",
    "    for i in range(theDayCount):\n",
    "\n",
    "        # Get the date range\n",
    "        theStartDate = overallStartDate + datetime.timedelta(days=i)\n",
    "        theStart = datetime.datetime.strftime(theStartDate, \"%Y-%m-%d\")\n",
    "        theEndDate = overallStartDate + datetime.timedelta(days=i+1)\n",
    "        theEnd = datetime.datetime.strftime(theEndDate, \"%Y-%m-%d\")\n",
    "\n",
    "        # Create a dataframe to store daily results\n",
    "        dfCombo = pd.DataFrame()\n",
    "\n",
    "        # Try processing the day    \n",
    "        #try:\n",
    "\n",
    "        # Print the current time\n",
    "        hereNow = datetime.datetime.now().strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "        print(\"Processing period: \" + str(theStart) + \" at \" + hereNow)\n",
    "\n",
    "        # Create an image collection\n",
    "        theImgColl = get_s2_sr_cld_col(geeFeatureGeometry, theStart, theEnd)\n",
    "        numElements = theImgColl.size().getInfo()\n",
    "\n",
    "        # If images are found for that day\n",
    "        if numElements > 0:\n",
    "\n",
    "            # Print the number of images found\n",
    "            print(\"Images found: \" + str(numElements))\n",
    "\n",
    "            # Add cloud shadow mask\n",
    "            theImgCollExpanded = theImgColl.map(add_cld_shdw_mask)\n",
    "\n",
    "            # Calculate stats from layers\n",
    "            statsDict = calc_stats_from_layers(theImgCollExpanded, geeFeatureGeometry, json_features, utmZoneInfo,\n",
    "                                                                   mdwiThresh, ndviThresh)            \n",
    "            # Retrieve stats for allAreas\n",
    "            dfAll = retrieveStatsFromJSON(statsDict[\"allAreas\"], uniqueid_field, \"sum\", totAreaString)\n",
    "\n",
    "            # Add date column\n",
    "            dfAll[dateString] = theStart\n",
    "\n",
    "            # Retrieve stats for MDWI and NDVI, and Clouds\n",
    "            dfMDWI = retrieveStatsFromJSON(statsDict[\"mndwiAreas\"], uniqueid_field, \"sum\", mdwiAreaString)\n",
    "            dfNDVI = retrieveStatsFromJSON(statsDict[\"ndviAreas\"], uniqueid_field, \"sum\", ndviAreaString)\n",
    "            dfMDWIAvg = retrieveStatsFromJSON(statsDict[\"mndwiAvg\"], uniqueid_field, \"mean\", mdwiAvgString)\n",
    "            dfNDVIAvg = retrieveStatsFromJSON(statsDict[\"ndviAvg\"], uniqueid_field, \"mean\", ndviAvgString)\n",
    "            dfNDMIAvg = retrieveStatsFromJSON(statsDict[\"ndmiAvg\"], uniqueid_field, \"mean\", ndmiAvgString)\n",
    "            dfCloud = retrieveStatsFromJSON(statsDict[\"cloudAreas\"], uniqueid_field, \"sum\", cloudAreaString)\n",
    "\n",
    "            #Merge all of the DF's together\n",
    "            dfCombo = pd.merge(dfAll, dfMDWI, how=\"left\", left_on=[uniqueid_field], right_on=[uniqueid_field])\n",
    "            dfCombo = pd.merge(dfCombo, dfMDWIAvg, how=\"left\", left_on=[uniqueid_field], right_on=[uniqueid_field])\n",
    "            dfCombo = pd.merge(dfCombo, dfNDVI, how=\"left\", left_on=[uniqueid_field], right_on=[uniqueid_field])\n",
    "            dfCombo = pd.merge(dfCombo, dfNDVIAvg, how=\"left\", left_on=[uniqueid_field], right_on=[uniqueid_field])\n",
    "            dfCombo = pd.merge(dfCombo, dfNDMIAvg, how=\"left\", left_on=[uniqueid_field], right_on=[uniqueid_field])\n",
    "            dfCombo = pd.merge(dfCombo, dfCloud, how=\"left\", left_on=[uniqueid_field], right_on=[uniqueid_field])\n",
    "\n",
    "            # Add stats to the results dataframe\n",
    "            if len(dfCombo) > 0:\n",
    "                tsStats = pd.concat([tsStats, dfCombo])\n",
    "\n",
    "            # Print the finish time for the day\n",
    "            hereNow = datetime.datetime.now().strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "            print(\"Done processing day: \" + str(theStart) + \" at \" + hereNow)\n",
    "            print(\"\")\n",
    "\n",
    "        else:\n",
    "            # Print the finish time for the day\n",
    "            print(\"No images found\")\n",
    "            print(\"\")   \n",
    "\n",
    "        #except:\n",
    "\n",
    "            # Print the finish time for the day\n",
    "            #rightNow = datetime.datetime.now().strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "            #print(\"Failed processing day: \" + str(theStart) + \" at \" + rightNow)\n",
    "            #print(\"\")\n",
    "\n",
    "# Save total results to CSV\n",
    "tsStats.to_csv(outputCSV, index=False)\n",
    "\n",
    "# Calculate time taken for completion\n",
    "rightNow = datetime.datetime.now().strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "print(\"Entire period processed, started at \" + beginNow + \" and finished at \" + rightNow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
